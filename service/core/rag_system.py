"""
RAG ì‹œìŠ¤í…œ í†µí•© ì»´í¬ë„ŒíŠ¸
ê²€ìƒ‰ ì¦ê°• ìƒì„±(Retrieval-Augmented Generation) ê¸°ëŠ¥ì„ ì œê³µ
EXAONE-3.5-2.4B-Instruct ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ ë° MongoDB ë¡œê¹… í¬í•¨
"""

import logging
import asyncio
from typing import List, Dict, Any
from dataclasses import dataclass

from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from langchain_huggingface import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
from service.storage.vector_store import search_similar_documents
from service.storage.rag_logger import evaluate_and_log_rag, quick_rag_score, RAGEvaluationResult
from shared.config.settings import settings
from pydantic import BaseModel

logger = logging.getLogger(__name__)

@dataclass
class RAGConfig:
    """RAG ì‹œìŠ¤í…œ ì„¤ì •"""
    embedding_model: str = settings.model.embedding_model  # settingsì—ì„œ ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    llm_model: str = settings.model.exaone_model_path  # settingsì—ì„œ EXAONE ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    max_retrieved_docs: int = 5 # ìµœëŒ€ ëª‡ ê°œì˜ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¬ì§€ ì§€ì •
    similarity_threshold: float = 0.3 # ê²€ìƒ‰ëœ ë¬¸ì„œ ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ì°¸ì¡°
    max_context_length: int = 4000 # LLMì— ì „ë‹¬í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ì˜ ìµœì¢… ê¸¸ì´
    max_new_tokens: int = 1000  # ë‹µë³€ ì‹œ ìƒˆë¡œ ìƒì„±í•  ìµœëŒ€ í† í° ê°œìˆ˜
    temperature: float = 0.1  # ìƒì„± ì˜¨ë„

class RAGResponse(BaseModel):
    """RAG ì‘ë‹µ ëª¨ë¸"""
    answer: str
    retrieved_documents: List[Dict[str, Any]]
    context_used: str
    confidence_score: float
    quality_score: float = 0.0  # GPT-4.1 í’ˆì§ˆ í‰ê°€ ì ìˆ˜ (0-10)
    evaluation_result: Dict[str, Any] = None  # ìƒì„¸ í‰ê°€ ê²°ê³¼

class RAGSystem:
    """RAG ì‹œìŠ¤í…œ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config: RAGConfig = None):
        self.config = config or RAGConfig()
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (settingsì—ì„œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸°)
        self.embed_model = HuggingFaceEmbedding(
            model_name=settings.model.embedding_model
        )
        
        # EXAONE ëª¨ë¸ê³¼ langchain íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        self._initialize_llm_pipeline()
        
        logger.info(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ - ì„ë² ë”©: {settings.model.embedding_model}, LLM: {settings.model.exaone_model_path}")
    
    def _initialize_llm_pipeline(self):
        """EXAONE ëª¨ë¸ê³¼ langchain íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        try:
            local_model_path = settings.model.exaone_model_path
            hf_model_name = settings.model.exaone_model_name
            
            logger.info(f"EXAONE ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘ - ë¡œì»¬ê²½ë¡œ: {local_model_path}")
            
            # 1. ë¡œì»¬ ëª¨ë¸ ì¡´ì¬ í™•ì¸
            if self._check_model_exists(local_model_path):
                logger.info("ë¡œì»¬ ëª¨ë¸ ë°œê²¬. ë¡œì»¬ì—ì„œ ë¡œë”©...")
                model_source = local_model_path
            else:
                logger.info("ë¡œì»¬ ëª¨ë¸ ì—†ìŒ. HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ ì €ì¥...")
                
                # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
                if self._download_and_save_model(local_model_path, hf_model_name):
                    logger.info("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ. ë¡œì»¬ì—ì„œ ë¡œë”©...")
                    model_source = local_model_path
                else:
                    logger.warning("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨. HuggingFaceì—ì„œ ì§ì ‘ ë¡œë”©...")
                    model_source = hf_model_name
            
            logger.info(f"ëª¨ë¸ ì†ŒìŠ¤: {model_source}")
            
            # 2. í† í¬ë‚˜ì´ì € ë¡œë”©
            logger.info("EXAONE í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
            try:
                tokenizer = AutoTokenizer.from_pretrained(
                    model_source,
                    trust_remote_code=True,
                    use_fast=True  # Fast tokenizer ì‚¬ìš©
                )
                logger.info(f"í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ: {type(tokenizer).__name__}")
            except Exception as e:
                logger.warning(f"Fast tokenizer ë¡œë”© ì‹¤íŒ¨, slow tokenizer ì‹œë„: {e}")
                tokenizer = AutoTokenizer.from_pretrained(
                    model_source,
                    trust_remote_code=True,
                    use_fast=False  # Slow tokenizer í´ë°±
                )
                logger.info(f"Slow tokenizer ë¡œë”© ì™„ë£Œ: {type(tokenizer).__name__}")
            
            # í† í¬ë‚˜ì´ì € ì„¤ì •
            if tokenizer.pad_token is None:
                if tokenizer.eos_token is not None:
                    tokenizer.pad_token = tokenizer.eos_token
                    logger.info(f"pad_tokenì„ eos_tokenìœ¼ë¡œ ì„¤ì •: {tokenizer.eos_token}")
                else:
                    # í´ë°±: ì„ì˜ì˜ í† í° ì„¤ì •
                    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
                    logger.info("pad_tokenì„ [PAD]ë¡œ ì„¤ì •")
            
            logger.info(f"í† í¬ë‚˜ì´ì € ì„¤ì • ì™„ë£Œ - pad_token: {tokenizer.pad_token}, eos_token: {tokenizer.eos_token}")
            
            # 3. ëª¨ë¸ ë¡œë”©
            logger.info("EXAONE ëª¨ë¸ ë¡œë”© ì¤‘...")
            model = AutoModelForCausalLM.from_pretrained(
                model_source,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”©
                load_in_8bit=False,  # í•„ìš”ì‹œ Trueë¡œ ë³€ê²½ (ì–‘ìí™”)
                load_in_4bit=False   # í•„ìš”ì‹œ Trueë¡œ ë³€ê²½ (ë” ê°•í•œ ì–‘ìí™”)
            )
            
            # HuggingFace íŒŒì´í”„ë¼ì¸ ìƒì„±
            logger.info("HuggingFace íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")
            
            # í† í¬ë‚˜ì´ì € ID ê²€ì¦
            pad_token_id = getattr(tokenizer, 'pad_token_id', None)
            eos_token_id = getattr(tokenizer, 'eos_token_id', None)
            
            if pad_token_id is None:
                logger.warning("pad_token_idê°€ Noneì…ë‹ˆë‹¤. eos_token_idë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                pad_token_id = eos_token_id
            
            if eos_token_id is None:
                logger.warning("eos_token_idê°€ Noneì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©")
                eos_token_id = tokenizer.vocab_size - 1  # ë§ˆì§€ë§‰ í† í° ID ì‚¬ìš©
            
            logger.info(f"í† í° ID ì„¤ì • - pad_token_id: {pad_token_id}, eos_token_id: {eos_token_id}")
            
            # device_map="auto"ë¥¼ ì‚¬ìš©í•œ ê²½ìš° device ì¸ì ì œê±°
            pipeline_kwargs = {
                "model": model,
                "tokenizer": tokenizer,
                "max_new_tokens": self.config.max_new_tokens,
                "temperature": self.config.temperature,
                "do_sample": True,
                "pad_token_id": pad_token_id,
                "eos_token_id": eos_token_id,
                "return_full_text": False  # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸í•˜ê³  ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
            }
            
            # GPUê°€ ìˆê³  device_map="auto"ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ device ì„¤ì •
            if torch.cuda.is_available():
                # accelerateë¡œ ë¡œë”©ëœ ëª¨ë¸ì¸ì§€ í™•ì¸
                if hasattr(model, 'hf_device_map') and model.hf_device_map:
                    logger.info("ëª¨ë¸ì´ accelerateë¡œ ë¡œë”©ë¨. device ì¸ì ì œê±°")
                else:
                    pipeline_kwargs["device"] = 0
                    logger.info("GPUì—ì„œ ì‹¤í–‰ - device ì„¤ì •")
            else:
                pipeline_kwargs["device"] = -1
                logger.info("CPUì—ì„œ ì‹¤í–‰ - device ì„¤ì •")
            
            hf_pipeline = pipeline("text-generation", **pipeline_kwargs)
            
            logger.info("íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ")
            
            # LangChain HuggingFacePipeline ë˜í¼
            self.llm = HuggingFacePipeline(pipeline=hf_pipeline)
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
            prompt_template = """ë‹¹ì‹ ì€ íšŒì‚¬ ë¬¸ì„œ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì •ë¦¬ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

            ë‹µë³€ ì‘ì„± ê°€ì´ë“œë¼ì¸:
            1. í•µì‹¬ ì •ë³´ë¥¼ ëª…í™•í•˜ê²Œ ì •ë¦¬í•˜ì—¬ ë‹µë³€
            2. êµ¬ì²´ì ì¸ ë‚´ìš© (ì‹œê°„, ì¥ì†Œ, ì—°ë½ì²˜, ì ˆì°¨ ë“±)ì„ ë¹ ëœ¨ë¦¬ì§€ ë§ê³  í¬í•¨
            3. ë¶ˆí•„ìš”í•œ ë°˜ë³µì´ë‚˜ ì¤‘ë³µ ë‚´ìš©ì€ ì œê±°
            4. ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ êµ¬ì¡°í™”í•˜ì—¬ ì œì‹œ
            5. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ

            ì§ˆë¬¸: {question}

            ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:
            {context}

            ë‹µë³€:"""

            self.prompt = PromptTemplate(
                input_variables=["question", "context"],
                template=prompt_template
            )
            
            # ìµœì‹  LangChain ë°©ì‹ìœ¼ë¡œ ì²´ì¸ ìƒì„± (prompt | llm | output_parser)
            output_parser = StrOutputParser()
            self.llm_chain = self.prompt | self.llm | output_parser
            
            # ë””ë°”ì´ìŠ¤ ì •ë³´ í™•ì¸
            device_info = "auto (accelerate)" if torch.cuda.is_available() and hasattr(model, 'hf_device_map') and model.hf_device_map else pipeline_kwargs.get('device', 'unknown')
            logger.info(f"EXAONE ëª¨ë¸ê³¼ LangChain íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ (Device: {device_info})")
            
        except Exception as e:
            logger.error(f"EXAONE ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            # í´ë°±ìœ¼ë¡œ None ì„¤ì • (í´ë°± ë‹µë³€ ì‚¬ìš©)
            self.llm = None
            self.llm_chain = None
    
    async def query(self, question: str, max_docs: int = None) -> RAGResponse:
        """
        RAG ì¿¼ë¦¬ ì‹¤í–‰
        
        Args:
            question (str): ì‚¬ìš©ì ì§ˆë¬¸
            max_docs (int): ìµœëŒ€ ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜
            
        Returns:
            RAGResponse: RAG ì‘ë‹µ
        """
        try:
            max_docs = max_docs or self.config.max_retrieved_docs
            
            # 1. ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
            question_embedding = self.embed_model.get_text_embedding(question)
            
            # 2. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ë” ë§ì´ ê²€ìƒ‰í•´ì„œ ì¤‘ë³µ ì œê±° í›„ í•„í„°ë§)
            similar_docs = await search_similar_documents(
                query_embedding=question_embedding,
                limit=max_docs * 3  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ë” ë§ì´ ê²€ìƒ‰
            )
            
            # 3. ê°™ì€ source_file ì¤‘ë³µ ì œê±° ë° í†µí•©
            deduplicated_docs = self._deduplicate_by_source_file(similar_docs)
            
            # 4. ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ (ìœ ì‚¬ë„ ì„ê³„ê°’)
            filtered_docs = [
                doc for doc in deduplicated_docs 
                if doc.get('score', 0) >= self.config.similarity_threshold
            ]
            
            # 5. ìš”ì²­ëœ ë¬¸ì„œ ìˆ˜ë¡œ ì œí•œ
            filtered_docs = filtered_docs[:max_docs]
            
            # 6. ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = self._build_context(filtered_docs)
            
            # 7. ë‹µë³€ ìƒì„± (EXAONE ëª¨ë¸ê³¼ LangChainì„ í™œìš©í•œ ì •ë¦¬ëœ ë‹µë³€)
            answer = await self._generate_answer(question, context, filtered_docs)
            
            # 8. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence = self._calculate_confidence(filtered_docs)
            
            # 9. RAG í’ˆì§ˆ í‰ê°€ ë° MongoDB ë¡œê¹… (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
            quality_score = 0.0
            evaluation_result = None
            
            try:
                # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ í’ˆì§ˆ í‰ê°€ ë° ë¡œê¹… ì‹¤í–‰
                if context and answer:
                    asyncio.create_task(
                        self._evaluate_and_log_quality(question, context, answer)
                    )
                    
                    # ë¹ ë¥¸ í’ˆì§ˆ ì ìˆ˜ë§Œ ê°€ì ¸ì˜¤ê¸° (ë¸”ë¡œí‚¹í•˜ì§€ ì•ŠìŒ)
                    quality_score = await quick_rag_score(question, context, answer)
                    
            except Exception as e:
                logger.warning(f"RAG í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
            
            return RAGResponse(
                answer=answer,
                retrieved_documents=filtered_docs,
                context_used=context,
                confidence_score=confidence,
                quality_score=quality_score
            )
            
        except Exception as e:
            logger.error(f"RAG ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return RAGResponse(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                retrieved_documents=[],
                context_used="",
                confidence_score=0.0
            )
    
    def _deduplicate_by_source_file(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê°™ì€ source_fileì˜ ë¬¸ì„œë“¤ì„ ì¤‘ë³µ ì œê±°í•˜ê³  í†µí•©"""
        if not documents:
            return []
        
        # source_fileë³„ë¡œ ê·¸ë£¹í™”
        file_groups = {}
        for doc in documents:
            source_file = doc.get('source_file', 'unknown')
            if source_file not in file_groups:
                file_groups[source_file] = []
            file_groups[source_file].append(doc)
        
        # ê° íŒŒì¼ë³„ë¡œ ìµœê³  ì ìˆ˜ ì²­í¬ë¥¼ ëŒ€í‘œë¡œ ì„ íƒí•˜ê³  í…ìŠ¤íŠ¸ í†µí•©
        deduplicated = []
        for source_file, docs in file_groups.items():
            # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
            docs.sort(key=lambda x: x.get('score', 0), reverse=True)
            
            # ìµœê³  ì ìˆ˜ ë¬¸ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ í†µí•©
            representative_doc = docs[0].copy()
            
            # ìƒìœ„ 5ê°œ ì²­í¬ì˜ í…ìŠ¤íŠ¸ë¥¼ ê²°í•© (ë” ë§ì€ ì •ë³´ í¬í•¨)
            combined_texts = []
            for doc in docs[:5]:
                text = doc.get('text', '').strip()
                if text and text not in combined_texts:
                    combined_texts.append(text)
            
            # í…ìŠ¤íŠ¸ ê²°í•©
            combined_text = '\n\n'.join(combined_texts)
            representative_doc['text'] = combined_text
            representative_doc['combined_chunks'] = len(docs)
            
            deduplicated.append(representative_doc)
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
        deduplicated.sort(key=lambda x: x.get('score', 0), reverse=True)
        return deduplicated
    
    def _build_context(self, documents: List[Dict[str, Any]]) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        if not documents:
            return ""
        
        context_parts = []
        current_length = 0
        
        for i, doc in enumerate(documents):
            doc_text = doc.get('text', '')
            source_file = doc.get('source_file', 'unknown')
            
            # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ í™•ì¸
            if current_length + len(doc_text) > self.config.max_context_length:
                break
            
            context_part = f"[ë¬¸ì„œ {i+1} - {source_file}]\n{doc_text}\n"
            context_parts.append(context_part)
            current_length += len(context_part)
        
        return "\n".join(context_parts)
    
    async def _generate_answer(self, question: str, context: str, documents: List[Dict[str, Any]]) -> str:
        """
        EXAONE ëª¨ë¸ê³¼ LangChainì„ í™œìš©í•œ ë‹µë³€ ìƒì„±
        
        Args:
            question (str): ì‚¬ìš©ì ì§ˆë¬¸
            context (str): ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
            documents (List[Dict[str, Any]]): ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
            
        Returns:
            str: ìƒì„±ëœ ë‹µë³€
        """
        if not documents:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œê°€ ì¸ë±ì‹±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        try:
            # EXAONE ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if self.llm_chain is None:
                logger.warning("EXAONE ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ. í´ë°± ë‹µë³€ ì‚¬ìš©")
                return self._generate_fallback_answer(context, documents)
            
            # LangChainì„ í†µí•œ ë‹µë³€ ìƒì„±
            logger.info("EXAONE ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
            
            # ìµœì‹  LangChain ë°©ì‹ìœ¼ë¡œ invoke ì‚¬ìš©
            answer = await asyncio.to_thread(
                self.llm_chain.invoke,
                {"question": question, "context": context}
            )
            
            # ë‹µë³€ í›„ì²˜ë¦¬
            answer = answer.strip() if isinstance(answer, str) else str(answer).strip()
            
            # í”„ë¡¬í”„íŠ¸ ë°˜ë³µì´ë‚˜ ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°
            if "ë‹µë³€:" in answer:
                answer = answer.split("ë‹µë³€:")[-1].strip()
            
            # ë‹µë³€ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ í´ë°±
            if len(answer) < 20:
                logger.warning("ìƒì„±ëœ ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ. í´ë°± ë‹µë³€ ì‚¬ìš©")
                return self._generate_fallback_answer(context, documents)
            
            logger.info("EXAONE ëª¨ë¸ ë‹µë³€ ìƒì„± ì™„ë£Œ")
            return answer
            
        except Exception as e:
            logger.warning(f"EXAONE ëª¨ë¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ (í´ë°± ì‚¬ìš©): {e}")
            return self._generate_fallback_answer(context, documents)
    
    def _generate_fallback_answer(self, context: str, documents: List[Dict[str, Any]]) -> str:
        """EXAONE ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  í´ë°± ë‹µë³€ ìƒì„±"""
        if not documents:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ
        context_lines = context.split('\n')
        answer_lines = []
        
        for line in context_lines:
            line = line.strip()
            # ë¬¸ì„œ í—¤ë” ë¼ì¸ì€ ì œì™¸
            if line.startswith('[ë¬¸ì„œ') or not line:
                continue
            # í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨
            if len(line) > 10:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ ë‚´ìš©ë§Œ
                answer_lines.append(line)
        
        if answer_lines:
            # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
            unique_lines = []
            for line in answer_lines:
                if line not in unique_lines:
                    unique_lines.append(line)
            
            return '\n\n'.join(unique_lines[:5])  # ìµœëŒ€ 5ê°œ í•µì‹¬ ì •ë³´
        else:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ì§€ë§Œ ì ì ˆí•œ ë‹µë³€ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _calculate_confidence(self, documents: List[Dict[str, Any]]) -> float:
        """ê²€ìƒ‰ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ì‹ ë¢°ë„ ëŒ€ì‹  ë‹¨ìˆœ í’ˆì§ˆ ì ìˆ˜)"""
        if not documents:
            return 0.0
        
        # í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ë°˜ í’ˆì§ˆ ì ìˆ˜
        scores = [doc.get('score', 0) for doc in documents]
        avg_score = sum(scores) / len(scores)
        
        # ë¬¸ì„œ ìˆ˜ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
        doc_count_weight = min(len(documents) / self.config.max_retrieved_docs, 1.0)
        
        # ìµœì¢… í’ˆì§ˆ ì ìˆ˜ (0~1 ë²”ìœ„)
        quality_score = avg_score * doc_count_weight
        return round(quality_score, 2)
    
    async def _evaluate_and_log_quality(self, user_prompt: str, context: str, answer: str):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ RAG í’ˆì§ˆ í‰ê°€ ë° MongoDB ë¡œê¹…"""
        try:
            logger.info("ğŸ” ë°±ê·¸ë¼ìš´ë“œ RAG í’ˆì§ˆ í‰ê°€ ì‹œì‘")
            
            # GPT-4.1ì„ ì´ìš©í•œ í’ˆì§ˆ í‰ê°€ ë° MongoDB ë¡œê¹…
            evaluation_result = await evaluate_and_log_rag(
                user_prompt=user_prompt,
                rag_context=context,
                rag_answer=answer
            )
            
            logger.info(
                f"âœ… RAG í’ˆì§ˆ í‰ê°€ ì™„ë£Œ - "
                f"ì „ì²´ì ìˆ˜: {evaluation_result.overall_score}/10, "
                f"ê´€ë ¨ì„±: {evaluation_result.relevance_score}/10, "
                f"ì •í™•ì„±: {evaluation_result.accuracy_score}/10, "
                f"ì™„ì„±ë„: {evaluation_result.completeness_score}/10"
            )
            
        except Exception as e:
            logger.error(f"âŒ ë°±ê·¸ë¼ìš´ë“œ RAG í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def get_system_status(self) -> Dict[str, Any]:
        """RAG ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
        try:
            # ì¸ë±ì„œ ì •ë³´
            indexer = get_document_indexer()
            indexer_info = indexer.get_indexer_info()
            
            # ë²¡í„° ìŠ¤í† ì–´ ì •ë³´
            vector_info = indexer_info.get('vector_store_info', {})
            
            # EXAONE ëª¨ë¸ ìƒíƒœ í™•ì¸
            llm_status = "initialized" if self.llm_chain is not None else "failed"
            
            return {
                "rag_config": {
                    "embedding_model": settings.model.embedding_model,  # settingsì—ì„œ ê°€ì ¸ì˜¤ê¸°
                    "llm_model": settings.model.exaone_model_path,      # settingsì—ì„œ ê°€ì ¸ì˜¤ê¸°
                    "max_retrieved_docs": self.config.max_retrieved_docs,
                    "similarity_threshold": self.config.similarity_threshold,
                    "max_context_length": self.config.max_context_length,
                    "max_new_tokens": self.config.max_new_tokens,
                    "temperature": self.config.temperature
                },
                "llm_status": llm_status,  # LLM ìƒíƒœ ì •ë³´ ì¶”ê°€
                "indexer_status": indexer_info,
                "total_indexed_documents": vector_info.get('total_entities', 0),
                "status": "active" if vector_info.get('total_entities', 0) > 0 else "no_documents"
            }
            
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"status": "error", "message": str(e)}

    def _download_and_save_model(self, local_path: str, model_name: str):
        """EXAONE ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œì»¬ì— ì €ì¥"""
        try:
            import os
            from pathlib import Path
            
            logger.info(f"EXAONE ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_name} -> {local_path}")
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            Path(local_path).mkdir(parents=True, exist_ok=True)
            
            # í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
            logger.info("í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            tokenizer.save_pretrained(local_path)
            logger.info(f"í† í¬ë‚˜ì´ì € ì €ì¥ ì™„ë£Œ: {local_path}")
            
            # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
            logger.info("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            model.save_pretrained(local_path)
            logger.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {local_path}")
            
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    
    def _check_model_exists(self, local_path: str) -> bool:
        """ë¡œì»¬ ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            import os
            from pathlib import Path
            
            model_path = Path(local_path)
            
            # í•„ìˆ˜ íŒŒì¼ë“¤ í™•ì¸
            required_files = [
                "config.json",
                "tokenizer.json",
                "tokenizer_config.json"
            ]
            
            # pytorch ëª¨ë¸ íŒŒì¼ í™•ì¸ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì´ë¦„)
            model_files = [
                "pytorch_model.bin",
                "model.safetensors", 
                "pytorch_model-00001-of-00001.bin"
            ]
            
            # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸
            for file in required_files:
                if not (model_path / file).exists():
                    logger.warning(f"í•„ìˆ˜ íŒŒì¼ ì—†ìŒ: {file}")
                    return False
            
            # ëª¨ë¸ íŒŒì¼ ì¤‘ í•˜ë‚˜ëŠ” ì¡´ì¬í•´ì•¼ í•¨
            model_file_exists = any((model_path / file).exists() for file in model_files)
            if not model_file_exists:
                logger.warning("ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
                return False
            
            logger.info(f"ë¡œì»¬ ëª¨ë¸ í™•ì¸ ì™„ë£Œ: {local_path}")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì¡´ì¬ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

class SimpleRAGSystem:
    """ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œ (LLM ì—†ëŠ” ë²„ì „)"""
    
    def __init__(self):
        self.rag_system = RAGSystem()
    
    async def ask(self, question: str) -> str:
        """ê°„ë‹¨í•œ ì§ˆì˜ì‘ë‹µ"""
        response = await self.rag_system.query(question)
        return response.answer
    
    async def ask_with_details(self, question: str) -> Dict[str, Any]:
        """ìƒì„¸ ì •ë³´ì™€ í•¨ê»˜ ì§ˆì˜ì‘ë‹µ"""
        response = await self.rag_system.query(question)
        return {
            "answer": response.answer,
            "confidence": response.confidence_score,
            "sources": [doc.get('source_file') for doc in response.retrieved_documents],
            "retrieved_count": len(response.retrieved_documents)
        }

# ì§€ì—° ë¡œë”©ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
_rag_system = None
_simple_rag = None

def get_rag_system() -> RAGSystem:
    """RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì§€ì—° ë¡œë”©)"""
    global _rag_system
    
    if _rag_system is None:
        logger.info("ê¸°ë³¸ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        _rag_system = RAGSystem()
        logger.info("ê¸°ë³¸ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    return _rag_system

def get_simple_rag() -> SimpleRAGSystem:
    """Simple RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì§€ì—° ë¡œë”©)"""
    global _simple_rag
    
    if _simple_rag is None:
        logger.info("Simple RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        _simple_rag = SimpleRAGSystem()
        logger.info("Simple RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    return _simple_rag

# í¸ì˜ í•¨ìˆ˜ë“¤
async def rag_query(question: str, max_docs: int = None) -> RAGResponse:
    """RAG ì¿¼ë¦¬ í¸ì˜ í•¨ìˆ˜"""
    system = get_rag_system()
    return await system.query(question, max_docs)

async def simple_ask(question: str) -> str:
    """ê°„ë‹¨í•œ ì§ˆì˜ì‘ë‹µ í¸ì˜ í•¨ìˆ˜"""
    simple_rag = get_simple_rag()
    return await simple_rag.ask(question)

async def ask_with_context(question: str) -> Dict[str, Any]:
    """ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì§ˆì˜ì‘ë‹µ í¸ì˜ í•¨ìˆ˜"""
    simple_rag = get_simple_rag()
    return await simple_rag.ask_with_details(question)

async def get_rag_status() -> Dict[str, Any]:
    """RAG ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ í¸ì˜ í•¨ìˆ˜"""
    try:
        system = get_rag_system()
        return await system.get_system_status()
    except Exception as e:
        return {
            "status": "error", 
            "message": str(e),
            "rag_system": "ì´ˆê¸°í™” í•„ìš”"
        } 